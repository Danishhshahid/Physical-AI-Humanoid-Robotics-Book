"use strict";(self.webpackChunkphysical_ai_humanoid_robotics_textbook=self.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[194],{4368:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>p,frontMatter:()=>i,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"appendices/appendix-a","title":"Appendix A: RAG Chatbot Setup","description":"Complete guide to setting up a Retrieval-Augmented Generation (RAG) chatbot for this textbook using Qdrant and Ollama.","source":"@site/docs/appendices/appendix-a.mdx","sourceDirName":"appendices","slug":"/appendices/appendix-a","permalink":"/docs/appendices/appendix-a","draft":false,"unlisted":false,"editUrl":"https://github.com/Danishhshahid/Physical-AI-Humanoid-Robotics-Book/tree/main/website/docs/appendices/appendix-a.mdx","tags":[],"version":"current","frontMatter":{"id":"appendix-a","title":"Appendix A: RAG Chatbot Setup","sidebar_label":"Appendix A: RAG Setup","description":"Complete guide to setting up a Retrieval-Augmented Generation (RAG) chatbot for this textbook using Qdrant and Ollama."},"sidebar":"tutorialSidebar","previous":{"title":"Ch6: AI-Robot Pipeline","permalink":"/docs/chapters/ch6-capstone"},"next":{"title":"Appendix B: Deployment","permalink":"/docs/appendices/appendix-b"}}');var s=r(4848),o=r(8453);const i={id:"appendix-a",title:"Appendix A: RAG Chatbot Setup",sidebar_label:"Appendix A: RAG Setup",description:"Complete guide to setting up a Retrieval-Augmented Generation (RAG) chatbot for this textbook using Qdrant and Ollama."},a=void 0,c={},d=[{value:"Appendix A: RAG Chatbot Setup",id:"appendix-a-rag-chatbot-setup",level:2},{value:"What is RAG?",id:"what-is-rag",level:3},{value:"Architecture",id:"architecture",level:3},{value:"Components",id:"components",level:3},{value:"Setup: Step-by-Step",id:"setup-step-by-step",level:3},{value:"Step 1: Install Qdrant (Vector Database)",id:"step-1-install-qdrant-vector-database",level:4},{value:"Step 2: Prepare Textbook Embeddings",id:"step-2-prepare-textbook-embeddings",level:4},{value:"Step 3: Build RAG API",id:"step-3-build-rag-api",level:4},{value:"Step 4: React Component for Docusaurus",id:"step-4-react-component-for-docusaurus",level:4},{value:"Deployment",id:"deployment",level:3},{value:"Troubleshooting",id:"troubleshooting",level:3}];function l(e){const n={a:"a",code:"code",h2:"h2",h3:"h3",h4:"h4",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h2,{id:"appendix-a-rag-chatbot-setup",children:"Appendix A: RAG Chatbot Setup"}),"\n",(0,s.jsxs)(n.p,{children:["This appendix shows how to build a ",(0,s.jsx)(n.strong,{children:"Retrieval-Augmented Generation (RAG) chatbot"}),' that answers questions about this textbook. Users can ask "How do I compute forward kinematics?" and the chatbot retrieves relevant sections and generates answers using Llama 3.']}),"\n",(0,s.jsx)(n.h3,{id:"what-is-rag",children:"What is RAG?"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"RAG"})," (Retrieval-Augmented Generation) = Search + Generation"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Retrieval"}),": Find relevant textbook sections using semantic search"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Augmentation"}),": Add retrieved sections to the LLM prompt"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Generation"}),": LLM generates answer with textbook context"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Advantage"}),": Answers are grounded in the textbook, not hallucinated."]}),"\n",(0,s.jsx)(n.h3,{id:"architecture",children:"Architecture"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-mermaid",children:'graph LR\r\n    User["\ud83d\udc64 User Query<br/>(What is kinematics?)"]\r\n    Encode["\ud83d\udd0d Encode Query<br/>(CLIP Text Encoder)<br/>\u2192 512-D vector"]\r\n    Search["\ud83d\udcda Search Qdrant<br/>(Semantic Search)"]\r\n    Retrieve["\ud83d\udcc4 Retrieved Sections<br/>(Top 3)"]\r\n    Prompt["\ud83d\udcdd Augment Prompt<br/>(Query + Context)"]\r\n    LLM["\ud83e\udde0 Llama 3<br/>(Generate Answer)"]\r\n    Response["\ud83d\udcac Response<br/>(Grounded in textbook)"]\r\n\r\n    User --\x3e Encode\r\n    Encode --\x3e Search\r\n    Search --\x3e Retrieve\r\n    Retrieve --\x3e Prompt\r\n    Prompt --\x3e LLM\r\n    LLM --\x3e Response\n'})}),"\n",(0,s.jsx)(n.h3,{id:"components",children:"Components"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Component"}),(0,s.jsx)(n.th,{children:"Purpose"}),(0,s.jsx)(n.th,{children:"Technology"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Document Storage"})}),(0,s.jsx)(n.td,{children:"Store textbook sections"}),(0,s.jsx)(n.td,{children:"PostgreSQL or in-memory"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Vector Database"})}),(0,s.jsx)(n.td,{children:"Semantic search"}),(0,s.jsx)(n.td,{children:"Qdrant"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Embedding Model"})}),(0,s.jsx)(n.td,{children:"Convert text \u2192 vectors"}),(0,s.jsx)(n.td,{children:"CLIP ViT-B/32 or Sentence-BERT"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"LLM"})}),(0,s.jsx)(n.td,{children:"Generate answers"}),(0,s.jsx)(n.td,{children:"Llama 3 (via Ollama)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"API"})}),(0,s.jsx)(n.td,{children:"User interface"}),(0,s.jsx)(n.td,{children:"FastAPI (Python) or Node.js"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Frontend"})}),(0,s.jsx)(n.td,{children:"Web UI"}),(0,s.jsx)(n.td,{children:"React component for Docusaurus"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"setup-step-by-step",children:"Setup: Step-by-Step"}),"\n",(0,s.jsx)(n.h4,{id:"step-1-install-qdrant-vector-database",children:"Step 1: Install Qdrant (Vector Database)"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Option A: Docker (Recommended)"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"docker run -p 6333:6333 -p 6334:6334 qdrant/qdrant:latest\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Option B: Local Installation"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# macOS\r\nbrew install qdrant\r\n\r\n# Linux\r\ncurl https://qdrant.io/install.sh | sh\r\n\r\n# Start server\r\nqdrant_server\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Verify: ",(0,s.jsx)(n.a,{href:"http://localhost:6333",children:"http://localhost:6333"})]}),"\n",(0,s.jsx)(n.h4,{id:"step-2-prepare-textbook-embeddings",children:"Step 2: Prepare Textbook Embeddings"}),"\n",(0,s.jsxs)(n.p,{children:["Script: ",(0,s.jsx)(n.code,{children:"ingest_docs.py"})]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import os\r\nimport json\r\nimport numpy as np\r\nfrom pathlib import Path\r\nimport clip\r\nimport torch\r\nfrom qdrant_client import QdrantClient\r\nfrom qdrant_client.models import Distance, VectorParams, PointStruct\r\nimport re\r\n\r\nclass TextbookIngester:\r\n    """Extract and embed textbook chapters into Qdrant."""\r\n\r\n    def __init__(self, qdrant_url: str = "http://localhost:6333"):\r\n        self.client = QdrantClient(url=qdrant_url)\r\n        self.model, self.preprocess = clip.load("ViT-B/32", device="cuda")\r\n        self.model.eval()\r\n        self.collection_name = "textbook"\r\n\r\n    def create_collection(self):\r\n        """Create Qdrant collection."""\r\n        self.client.recreate_collection(\r\n            collection_name=self.collection_name,\r\n            vectors_config=VectorParams(size=512, distance=Distance.COSINE)\r\n        )\r\n        print(f"\u2713 Created collection: {self.collection_name}")\r\n\r\n    def split_into_sections(self, text: str, max_chars: int = 2000) -> list:\r\n        """Split text into overlapping sections."""\r\n        sections = []\r\n        sentences = re.split(r\'(?<=[.!?])\\s+\', text)\r\n\r\n        current_section = ""\r\n        for sentence in sentences:\r\n            if len(current_section) + len(sentence) < max_chars:\r\n                current_section += sentence + " "\r\n            else:\r\n                if current_section:\r\n                    sections.append(current_section.strip())\r\n                current_section = sentence + " "\r\n\r\n        if current_section:\r\n            sections.append(current_section.strip())\r\n\r\n        return sections\r\n\r\n    @torch.no_grad()\r\n    def embed_text(self, text: str) -> np.ndarray:\r\n        """Encode text to 512-D vector."""\r\n        tokens = clip.tokenize(text[:76]).cuda()  # CLIP max token length\r\n        embedding = self.model.encode_text(tokens)\r\n        embedding = embedding / embedding.norm(dim=-1, keepdim=True)\r\n        return embedding.cpu().numpy()[0]\r\n\r\n    def ingest_chapter(self, chapter_path: str, chapter_name: str):\r\n        """Read chapter .mdx file and ingest sections."""\r\n\r\n        with open(chapter_path, \'r\') as f:\r\n            content = f.read()\r\n\r\n        # Strip frontmatter\r\n        if content.startswith(\'---\'):\r\n            _, _, content = content.split(\'---\', 2)\r\n\r\n        # Split into sections\r\n        sections = self.split_into_sections(content)\r\n\r\n        points = []\r\n        for i, section in enumerate(sections):\r\n            # Skip very short sections\r\n            if len(section) < 100:\r\n                continue\r\n\r\n            section_id = hash(section) % (2**31)  # Unique ID\r\n            embedding = self.embed_text(section)\r\n\r\n            point = PointStruct(\r\n                id=section_id,\r\n                vector=embedding.tolist(),\r\n                payload={\r\n                    "chapter": chapter_name,\r\n                    "section_num": i,\r\n                    "text": section[:1000],  # Store first 1000 chars\r\n                    "full_text": section\r\n                }\r\n            )\r\n            points.append(point)\r\n\r\n        # Upload batch to Qdrant\r\n        self.client.upsert(\r\n            collection_name=self.collection_name,\r\n            points=points\r\n        )\r\n        print(f"\u2713 Ingested {len(points)} sections from {chapter_name}")\r\n\r\n    def ingest_all_chapters(self, chapters_dir: str):\r\n        """Process all chapters."""\r\n        self.create_collection()\r\n\r\n        chapter_files = sorted(Path(chapters_dir).glob("ch*.mdx"))\r\n\r\n        for chapter_path in chapter_files:\r\n            chapter_name = chapter_path.stem\r\n            print(f"\\nProcessing {chapter_name}...")\r\n            self.ingest_chapter(str(chapter_path), chapter_name)\r\n\r\n        print(f"\\n\u2705 All chapters ingested: {len(chapter_files)} chapters")\r\n\r\n# Run ingestion\r\nif __name__ == "__main__":\r\n    ingester = TextbookIngester()\r\n    ingester.ingest_all_chapters("./website/docs/chapters/")\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Run it:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"python3 ingest_docs.py\n"})}),"\n",(0,s.jsxs)(n.p,{children:["This will ingest all chapters into Qdrant. Check: ",(0,s.jsx)(n.a,{href:"http://localhost:6333/dashboard",children:"http://localhost:6333/dashboard"})," (Qdrant admin UI)."]}),"\n",(0,s.jsx)(n.h4,{id:"step-3-build-rag-api",children:"Step 3: Build RAG API"}),"\n",(0,s.jsxs)(n.p,{children:["Script: ",(0,s.jsx)(n.code,{children:"rag_api.py"})]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from fastapi import FastAPI, HTTPException\r\nfrom pydantic import BaseModel\r\nimport clip\r\nimport torch\r\nimport numpy as np\r\nfrom qdrant_client import QdrantClient\r\nimport requests\r\nimport json\r\n\r\napp = FastAPI()\r\n\r\n# Initialize\r\nmodel, preprocess = clip.load("ViT-B/32", device="cuda")\r\nmodel.eval()\r\nqdrant_client = QdrantClient(url="http://localhost:6333")\r\n\r\nclass QueryRequest(BaseModel):\r\n    query: str\r\n\r\nclass QueryResponse(BaseModel):\r\n    query: str\r\n    answer: str\r\n    sources: list\r\n\r\n@app.post("/query", response_model=QueryResponse)\r\nasync def query_rag(req: QueryRequest):\r\n    """Query the RAG system."""\r\n\r\n    # Step 1: Embed query\r\n    query_tokens = clip.tokenize(req.query[:76]).cuda()\r\n    with torch.no_grad():\r\n        query_embedding = model.encode_text(query_tokens)\r\n        query_embedding = query_embedding / query_embedding.norm(dim=-1, keepdim=True)\r\n\r\n    query_vector = query_embedding.cpu().numpy()[0].tolist()\r\n\r\n    # Step 2: Search Qdrant\r\n    try:\r\n        search_results = qdrant_client.search(\r\n            collection_name="textbook",\r\n            query_vector=query_vector,\r\n            limit=3  # Top 3 results\r\n        )\r\n    except Exception as e:\r\n        raise HTTPException(status_code=500, detail=f"Search error: {str(e)}")\r\n\r\n    # Step 3: Extract context\r\n    context = "\\n\\n".join([\r\n        result.payload["full_text"]\r\n        for result in search_results\r\n    ])\r\n\r\n    sources = [\r\n        {\r\n            "chapter": result.payload["chapter"],\r\n            "score": result.score,\r\n            "excerpt": result.payload["text"][:200]\r\n        }\r\n        for result in search_results\r\n    ]\r\n\r\n    # Step 4: Augment prompt\r\n    prompt = f"""You are an expert roboticist teaching the "Physical AI & Humanoid Robotics Essentials" textbook.\r\n\r\nUSER QUESTION: {req.query}\r\n\r\nTEXTBOOK CONTEXT:\r\n{context}\r\n\r\nBased on the textbook context above, provide a clear, concise answer. Cite specific sections if relevant."""\r\n\r\n    # Step 5: Generate answer with Llama 3\r\n    response = requests.post(\r\n        "http://localhost:11434/api/generate",\r\n        json={\r\n            "model": "llama3",\r\n            "prompt": prompt,\r\n            "stream": False,\r\n            "temperature": 0.3\r\n        }\r\n    )\r\n\r\n    if response.status_code != 200:\r\n        raise HTTPException(status_code=500, detail="LLM error")\r\n\r\n    answer = response.json()["response"]\r\n\r\n    return QueryResponse(\r\n        query=req.query,\r\n        answer=answer,\r\n        sources=sources\r\n    )\r\n\r\n@app.get("/health")\r\nasync def health():\r\n    return {"status": "ok"}\r\n\r\nif __name__ == "__main__":\r\n    import uvicorn\r\n    uvicorn.run(app, host="0.0.0.0", port=8004)\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Run it:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"python3 rag_api.py\n"})}),"\n",(0,s.jsx)(n.p,{children:"Test:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'curl -X POST http://localhost:8004/query \\\r\n  -H "Content-Type: application/json" \\\r\n  -d \'{"query": "How do I compute forward kinematics?"}\'\n'})}),"\n",(0,s.jsx)(n.h4,{id:"step-4-react-component-for-docusaurus",children:"Step 4: React Component for Docusaurus"}),"\n",(0,s.jsxs)(n.p,{children:["Create: ",(0,s.jsx)(n.code,{children:"website/src/components/RagQuery.js"})]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-jsx",children:"import React, { useState } from 'react';\r\n\r\nexport default function RagQuery({ placeholder = \"Ask a question...\" }) {\r\n  const [query, setQuery] = useState('');\r\n  const [answer, setAnswer] = useState('');\r\n  const [sources, setSources] = useState([]);\r\n  const [loading, setLoading] = useState(false);\r\n  const [error, setError] = useState(null);\r\n\r\n  const handleSubmit = async (e) => {\r\n    e.preventDefault();\r\n    setLoading(true);\r\n    setError(null);\r\n\r\n    try {\r\n      const response = await fetch('/api/query', {\r\n        method: 'POST',\r\n        headers: { 'Content-Type': 'application/json' },\r\n        body: JSON.stringify({ query })\r\n      });\r\n\r\n      if (!response.ok) throw new Error('RAG query failed');\r\n\r\n      const data = await response.json();\r\n      setAnswer(data.answer);\r\n      setSources(data.sources);\r\n    } catch (err) {\r\n      setError(err.message);\r\n    } finally {\r\n      setLoading(false);\r\n    }\r\n  };\r\n\r\n  return (\r\n    <div style={{ marginTop: '20px', padding: '15px', backgroundColor: '#f5f5f5', borderRadius: '8px' }}>\r\n      <h4>\ud83d\udcda Ask the Textbook</h4>\r\n      <form onSubmit={handleSubmit}>\r\n        <input\r\n          type=\"text\"\r\n          value={query}\r\n          onChange={(e) => setQuery(e.target.value)}\r\n          placeholder={placeholder}\r\n          style={{\r\n            width: '100%',\r\n            padding: '10px',\r\n            borderRadius: '4px',\r\n            border: '1px solid #ccc',\r\n            marginBottom: '10px'\r\n          }}\r\n        />\r\n        <button type=\"submit\" disabled={loading} style={{ padding: '8px 16px' }}>\r\n          {loading ? 'Loading...' : 'Search'}\r\n        </button>\r\n      </form>\r\n\r\n      {error && <p style={{ color: 'red' }}>Error: {error}</p>}\r\n\r\n      {answer && (\r\n        <div style={{ marginTop: '15px' }}>\r\n          <h5>Answer:</h5>\r\n          <p>{answer}</p>\r\n          {sources.length > 0 && (\r\n            <div>\r\n              <h6>Sources:</h6>\r\n              <ul>\r\n                {sources.map((src, i) => (\r\n                  <li key={i}>\r\n                    <strong>{src.chapter}</strong> (score: {src.score.toFixed(2)})\r\n                    <br />\r\n                    <em>{src.excerpt}...</em>\r\n                  </li>\r\n                ))}\r\n              </ul>\r\n            </div>\r\n          )}\r\n        </div>\r\n      )}\r\n    </div>\r\n  );\r\n}\n"})}),"\n",(0,s.jsx)(n.p,{children:"Use in Docusaurus pages:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-mdx",children:"import RagQuery from '@site/src/components/RagQuery';\r\n\r\n<RagQuery placeholder=\"Ask me about humanoid kinematics\" />\n"})}),"\n",(0,s.jsx)(n.h3,{id:"deployment",children:"Deployment"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Docker Compose (Production)"})}),"\n",(0,s.jsxs)(n.p,{children:["Add to ",(0,s.jsx)(n.code,{children:"docker-compose.yml"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'qdrant:\r\n  image: qdrant/qdrant:latest\r\n  volumes:\r\n    - qdrant_data:/qdrant/storage\r\n  ports:\r\n    - "6333:6333"\r\n  networks:\r\n    - robot_network\r\n\r\nrag_api:\r\n  build:\r\n    context: .\r\n    dockerfile: Dockerfile.rag\r\n  depends_on:\r\n    - qdrant\r\n    - language  # Ollama service\r\n  environment:\r\n    QDRANT_URL: http://qdrant:6333\r\n    OLLAMA_HOST: http://language:11434\r\n  ports:\r\n    - "8004:8004"\r\n  networks:\r\n    - robot_network\n'})}),"\n",(0,s.jsx)(n.h3,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Issue"}),(0,s.jsx)(n.th,{children:"Solution"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Qdrant connection refused"}),(0,s.jsxs)(n.td,{children:["Check if Qdrant is running: ",(0,s.jsx)(n.code,{children:"curl http://localhost:6333"})]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Embedding mismatch"}),(0,s.jsx)(n.td,{children:"Ensure CLIP model and Qdrant vector size match (512)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Slow search"}),(0,s.jsxs)(n.td,{children:["Increase Qdrant ",(0,s.jsx)(n.code,{children:"index_size"})," or use GPU"]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"LLM not responding"}),(0,s.jsxs)(n.td,{children:["Verify Ollama is running: ",(0,s.jsx)(n.code,{children:"ollama pull llama3"})]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"React component not loading"}),(0,s.jsxs)(n.td,{children:["Rebuild Docusaurus: ",(0,s.jsx)(n.code,{children:"npm run build"})]})]})]})]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Next"}),": ",(0,s.jsx)(n.a,{href:"/docs/appendix-b",children:"Appendix B: Free-Tier Deployment \u2192"})]})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(l,{...e})}):l(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>i,x:()=>a});var t=r(6540);const s={},o=t.createContext(s);function i(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);