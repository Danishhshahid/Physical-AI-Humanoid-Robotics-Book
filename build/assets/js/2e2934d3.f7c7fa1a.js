"use strict";(self.webpackChunkphysical_ai_humanoid_robotics_textbook=self.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[177],{8453:(n,e,o)=>{o.d(e,{R:()=>i,x:()=>a});var t=o(6540);const s={},r=t.createContext(s);function i(n){const e=t.useContext(r);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:i(n.components),t.createElement(r.Provider,{value:e},n.children)}},8662:(n,e,o)=>{o.r(e),o.d(e,{assets:()=>l,contentTitle:()=>a,default:()=>m,frontMatter:()=>i,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"modules/m1-ros2/m1-python-agents","title":"Lesson 2: Bridging Python Agents to ROS Controllers","description":"Learn how to bridge AI agents (LLMs, planning algorithms) running in Python to actual ROS 2 robot controllers. From voice commands to executed trajectories.","source":"@site/docs/modules/m1-ros2/m1-python-agents.mdx","sourceDirName":"modules/m1-ros2","slug":"/modules/m1-ros2/m1-python-agents","permalink":"/docs/modules/m1-ros2/m1-python-agents","draft":false,"unlisted":false,"editUrl":"https://github.com/Danishhshahid/Physical-AI-Humanoid-Robotics-Book/tree/main/website/docs/modules/m1-ros2/m1-python-agents.mdx","tags":[],"version":"current","frontMatter":{"id":"m1-python-agents","title":"Lesson 2: Bridging Python Agents to ROS Controllers","sidebar_label":"L2: Python Agents to ROS","description":"Learn how to bridge AI agents (LLMs, planning algorithms) running in Python to actual ROS 2 robot controllers. From voice commands to executed trajectories."},"sidebar":"tutorialSidebar","previous":{"title":"L1: Nodes, Topics, Services","permalink":"/docs/modules/m1-ros2/m1-nodes-topics-services"},"next":{"title":"L3: URDF Humanoids","permalink":"/docs/modules/m1-ros2/m1-urdf-humanoids"}}');var s=o(4848),r=o(8453);const i={id:"m1-python-agents",title:"Lesson 2: Bridging Python Agents to ROS Controllers",sidebar_label:"L2: Python Agents to ROS",description:"Learn how to bridge AI agents (LLMs, planning algorithms) running in Python to actual ROS 2 robot controllers. From voice commands to executed trajectories."},a=void 0,l={},c=[{value:"The Agent-Controller Bridge",id:"the-agent-controller-bridge",level:2},{value:"Architecture: Agent Node",id:"architecture-agent-node",level:2},{value:"Example: Voice Command \u2192 Robot Arm Movement",id:"example-voice-command--robot-arm-movement",level:3},{value:"Using Local LLMs with Ollama",id:"using-local-llms-with-ollama",level:2},{value:"Safety Considerations",id:"safety-considerations",level:2},{value:"Real-World Example: Tesla Optimus",id:"real-world-example-tesla-optimus",level:2},{value:"Hands-On Project",id:"hands-on-project",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(n){const e={a:"a",code:"code",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.h2,{id:"the-agent-controller-bridge",children:"The Agent-Controller Bridge"}),"\n",(0,s.jsxs)(e.p,{children:["In modern humanoid robots, the ",(0,s.jsx)(e.strong,{children:"AI agent"})," and ",(0,s.jsx)(e.strong,{children:"robot controller"})," are separate systems that must communicate:"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:"\ud83e\udde0 AI Agent (Llama 3, GPT, etc.)\n   \u2193\n   [ROS 2 Bridge Node]\n   \u2193\n\ud83e\udd16 Robot Controller (ROS 2 nodes)\n   \u2193\n\u2699\ufe0f Actuators (Motors, Grippers)\n"})}),"\n",(0,s.jsx)(e.p,{children:"This lesson teaches you how to build that bridge."}),"\n",(0,s.jsx)(e.h2,{id:"architecture-agent-node",children:"Architecture: Agent Node"}),"\n",(0,s.jsxs)(e.p,{children:["An ",(0,s.jsx)(e.strong,{children:"agent node"})," is a ROS 2 node that:"]}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Listens"})," to natural language commands (voice, text, or LLM outputs)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Processes"})," them with an AI model"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Publishes"})," robot commands (joint angles, trajectories, gripper signals)"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"example-voice-command--robot-arm-movement",children:"Example: Voice Command \u2192 Robot Arm Movement"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Pose\nfrom trajectory_msgs.msg import JointTrajectory, JointTrajectoryPoint\nimport numpy as np\nfrom openai import OpenAI  # Or use local Llama 3 via Ollama\n\nclass VoiceCommandAgent(Node):\n    def __init__(self):\n        super().__init__(\'voice_command_agent\')\n\n        # Publisher to robot arm\n        self.arm_publisher = self.create_publisher(\n            JointTrajectory,\n            \'/arm/controller/command\',\n            10\n        )\n\n        # Subscribe to voice commands (from speech-to-text node)\n        self.voice_subscription = self.create_subscription(\n            String,\n            \'/voice/command\',\n            self.voice_callback,\n            10\n        )\n\n        # Initialize LLM client\n        self.client = OpenAI(api_key="your-key")\n\n        self.get_logger().info("Voice command agent initialized")\n\n    def voice_callback(self, msg):\n        """Called when a voice command is received."""\n        command = msg.data\n        self.get_logger().info(f"Received voice command: {command}")\n\n        # Step 1: Use LLM to translate voice to action\n        action = self.plan_action_with_llm(command)\n\n        # Step 2: Translate action to joint angles\n        trajectory = self.action_to_trajectory(action)\n\n        # Step 3: Publish to robot controller\n        self.arm_publisher.publish(trajectory)\n        self.get_logger().info(f"Executed action: {action}")\n\n    def plan_action_with_llm(self, command: str) -> dict:\n        """Use an LLM to parse the voice command."""\n        prompt = f"""\n        A robot hears the voice command: "{command}"\n\n        Respond with a JSON object describing the action:\n        {{\n            "action": "move_arm" | "grasp" | "push" | "place",\n            "target_position": {{"x": float, "y": float, "z": float}},\n            "gripper_state": "open" | "closed",\n            "duration_seconds": float\n        }}\n\n        Only output the JSON, nothing else.\n        """\n\n        response = self.client.chat.completions.create(\n            model="gpt-4",\n            messages=[{"role": "user", "content": prompt}],\n            temperature=0.3\n        )\n\n        import json\n        action_json = response.choices[0].message.content\n        action = json.loads(action_json)\n\n        return action\n\n    def action_to_trajectory(self, action: dict) -> JointTrajectory:\n        """Convert a semantic action to a joint trajectory."""\n        # This would typically use inverse kinematics\n        # For now, a simple example:\n\n        trajectory = JointTrajectory()\n        trajectory.header.stamp = self.get_clock().now().to_msg()\n        trajectory.joint_names = [\'shoulder_pan\', \'shoulder_lift\', \'elbow_flex\', \'wrist_1\', \'wrist_2\', \'wrist_3\']\n\n        # Simple IK: (would be much more complex in reality)\n        target_pos = action[\'target_position\']\n        q_target = self.simple_ik(target_pos[\'x\'], target_pos[\'y\'], target_pos[\'z\'])\n\n        point = JointTrajectoryPoint()\n        point.positions = q_target\n        point.time_from_start = rclpy.time.Duration(seconds=action[\'duration_seconds\']).to_msg()\n\n        trajectory.points.append(point)\n\n        return trajectory\n\n    def simple_ik(self, x, y, z):\n        """Simplified inverse kinematics for 6-DOF arm."""\n        # In reality, this would use a proper IK solver (e.g., scipy.optimize)\n        q = np.zeros(6)\n        q[0] = np.arctan2(y, x)  # Base rotation\n        # ... more complex computation ...\n        return q.tolist()\n\ndef main(args=None):\n    rclpy.init(args=args)\n    agent = VoiceCommandAgent()\n    rclpy.spin(agent)\n    agent.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(e.h2,{id:"using-local-llms-with-ollama",children:"Using Local LLMs with Ollama"}),"\n",(0,s.jsxs)(e.p,{children:["To avoid cloud API costs and latency, use ",(0,s.jsx)(e.strong,{children:"Ollama"})," to run Llama 3 locally:"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'import requests\nimport json\n\nclass LocalLlamaAgent(Node):\n    def __init__(self):\n        super().__init__(\'local_llama_agent\')\n        self.ollama_url = "http://localhost:11434/api/generate"\n\n    def plan_action_with_local_llm(self, command: str) -> dict:\n        """Use local Llama 3 via Ollama."""\n        prompt = f"""\n        A humanoid robot receives a voice command: "{command}"\n\n        Output a JSON object:\n        {{\n            "action": "move_arm" | "walk" | "pick_up" | "place",\n            "target": "location/object description",\n            "confidence": 0.0-1.0\n        }}\n        """\n\n        response = requests.post(\n            self.ollama_url,\n            json={\n                "model": "llama3",\n                "prompt": prompt,\n                "stream": False\n            }\n        )\n\n        # Parse response\n        output = response.json()[\'response\']\n\n        import json\n        action = json.loads(output)\n\n        return action\n'})}),"\n",(0,s.jsx)(e.h2,{id:"safety-considerations",children:"Safety Considerations"}),"\n",(0,s.jsxs)(e.p,{children:["When bridging AI agents to robot controllers, ",(0,s.jsx)(e.strong,{children:"safety is critical"}),":"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"class SafeAgentController(Node):\n    def __init__(self):\n        super().__init__('safe_agent_controller')\n\n        # Define safe workspace bounds\n        self.workspace_bounds = {\n            'x': (-0.5, 1.5),   # meters\n            'y': (-1.0, 1.0),\n            'z': (0.0, 2.0)\n        }\n\n        self.max_joint_velocity = 1.5  # rad/s\n\n    def is_action_safe(self, action: dict) -> bool:\n        \"\"\"Validate action before sending to robot.\"\"\"\n        target = action['target_position']\n\n        # Check workspace bounds\n        if not (self.workspace_bounds['x'][0] <= target['x'] <= self.workspace_bounds['x'][1]):\n            return False\n        if not (self.workspace_bounds['y'][0] <= target['y'] <= self.workspace_bounds['y'][1]):\n            return False\n        if not (self.workspace_bounds['z'][0] <= target['z'] <= self.workspace_bounds['z'][1]):\n            return False\n\n        # Check for dangerous keywords\n        dangerous_words = ['destroy', 'break', 'harm', 'hurt']\n        for word in dangerous_words:\n            if word in action.get('reasoning', '').lower():\n                return False\n\n        return True\n\n    def execute_action(self, action: dict):\n        \"\"\"Execute only if safe.\"\"\"\n        if not self.is_action_safe(action):\n            self.get_logger().error(f\"Action rejected: {action}\")\n            return\n\n        # Safe to execute\n        trajectory = self.action_to_trajectory(action)\n        self.arm_publisher.publish(trajectory)\n"})}),"\n",(0,s.jsx)(e.h2,{id:"real-world-example-tesla-optimus",children:"Real-World Example: Tesla Optimus"}),"\n",(0,s.jsx)(e.p,{children:"Tesla Optimus uses a similar architecture:"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Perception node"}),": Cameras \u2192 scene understanding"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Planning node"}),": LLM-based task planner"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Control node"}),": Converts planned actions to motor commands"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Safety monitor"}),": Ensures no collisions or constraint violations"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"hands-on-project",children:"Hands-On Project"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Build a robot arm controller that:"})}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"Listens to voice commands via a speech-to-text node"}),"\n",(0,s.jsx)(e.li,{children:"Uses Llama 3 (via Ollama) to interpret the command"}),"\n",(0,s.jsx)(e.li,{children:"Computes inverse kinematics to target position"}),"\n",(0,s.jsx)(e.li,{children:"Publishes a smooth trajectory to the robot"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Expected voice commands:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:'"Move the arm to the table"'}),"\n",(0,s.jsx)(e.li,{children:'"Pick up the red cube"'}),"\n",(0,s.jsx)(e.li,{children:'"Place the object in the bin"'}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Deliverables:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"A working Python ROS 2 node"}),"\n",(0,s.jsx)(e.li,{children:"Safety validation for all actions"}),"\n",(0,s.jsx)(e.li,{children:"Graceful error handling when the LLM response is invalid"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.a,{href:"/docs/modules/m1-ros2/m1-urdf-humanoids",children:"Lesson 3: URDF for Humanoids"})}),"\n",(0,s.jsx)(e.li,{children:"Experiment with different LLMs (Llama 2, Mistral, etc.)"}),"\n",(0,s.jsx)(e.li,{children:"Add multi-robot coordination (multiple arms, mobile base)"}),"\n"]})]})}function m(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(d,{...n})}):d(n)}}}]);