"use strict";(self.webpackChunkphysical_ai_humanoid_robotics_textbook=self.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[805],{4014:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>m,frontMatter:()=>i,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"modules/m3-isaac/m3-isaac-ros","title":"Lesson 2: Isaac ROS (GPU-Accelerated Perception)","description":"Implement hardware-accelerated perception with Isaac ROS: VSLAM, object detection, depth estimation.","source":"@site/docs/modules/m3-isaac/m3-isaac-ros.mdx","sourceDirName":"modules/m3-isaac","slug":"/modules/m3-isaac/m3-isaac-ros","permalink":"/docs/modules/m3-isaac/m3-isaac-ros","draft":false,"unlisted":false,"editUrl":"https://github.com/Danishhshahid/Physical-AI-Humanoid-Robotics-Book/tree/main/website/docs/modules/m3-isaac/m3-isaac-ros.mdx","tags":[],"version":"current","frontMatter":{"id":"m3-isaac-ros","title":"Lesson 2: Isaac ROS (GPU-Accelerated Perception)","sidebar_label":"L2: Isaac ROS","description":"Implement hardware-accelerated perception with Isaac ROS: VSLAM, object detection, depth estimation."},"sidebar":"tutorialSidebar","previous":{"title":"L1: Isaac Sim","permalink":"/docs/modules/m3-isaac/m3-isaac-sim"},"next":{"title":"L3: Nav2 Planning","permalink":"/docs/modules/m3-isaac/m3-nav2-planning"}}');var o=s(4848),a=s(8453);const i={id:"m3-isaac-ros",title:"Lesson 2: Isaac ROS (GPU-Accelerated Perception)",sidebar_label:"L2: Isaac ROS",description:"Implement hardware-accelerated perception with Isaac ROS: VSLAM, object detection, depth estimation."},r=void 0,c={},l=[{value:"What is Isaac ROS?",id:"what-is-isaac-ros",level:2},{value:"Installation",id:"installation",level:2},{value:"Visual SLAM (VSLAM)",id:"visual-slam-vslam",level:2},{value:"Launch VSLAM Node",id:"launch-vslam-node",level:3},{value:"Subscribe to Odometry",id:"subscribe-to-odometry",level:3},{value:"Real-Time Object Detection",id:"real-time-object-detection",level:2},{value:"Stereo Depth Estimation",id:"stereo-depth-estimation",level:2},{value:"Multi-Sensor Fusion",id:"multi-sensor-fusion",level:2},{value:"Performance Metrics",id:"performance-metrics",level:2},{value:"Next Lesson",id:"next-lesson",level:2}];function d(e){const n={a:"a",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h2,{id:"what-is-isaac-ros",children:"What is Isaac ROS?"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Isaac ROS"})," provides GPU-accelerated perception nodes for ROS 2:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["\ud83c\udfaf ",(0,o.jsx)(n.strong,{children:"VSLAM"}),": Visual SLAM using camera + IMU (10\xd7 faster than CPU)"]}),"\n",(0,o.jsxs)(n.li,{children:["\ud83d\udea8 ",(0,o.jsx)(n.strong,{children:"Object Detection"}),": Real-time YOLOv8 on GPU"]}),"\n",(0,o.jsxs)(n.li,{children:["\ud83d\udccf ",(0,o.jsx)(n.strong,{children:"Depth Estimation"}),": Stereo vision with GPU acceleration"]}),"\n",(0,o.jsxs)(n.li,{children:["\u26a1 ",(0,o.jsx)(n.strong,{children:"Hardware-optimized"}),": Runs on NVIDIA Jetson or data center GPUs"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"installation",children:"Installation"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Install Isaac ROS with VSLAM\nsudo apt-get install ros-humble-isaac-ros-vslam\n\n# Install perception packages\nsudo apt-get install ros-humble-isaac-ros-visual-slam \\\n                     ros-humble-isaac-ros-object-detection \\\n                     ros-humble-isaac-ros-dnn-stereo-depth\n"})}),"\n",(0,o.jsx)(n.h2,{id:"visual-slam-vslam",children:"Visual SLAM (VSLAM)"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"VSLAM"})," localizes the robot by tracking camera features:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"\ud83d\udcf7 Camera Feed \u2192 Feature Extraction \u2192 Feature Matching \u2192 Localization\n     \u2193\n     IMU (gyroscope) stabilizes rotation estimates\n"})}),"\n",(0,o.jsx)(n.h3,{id:"launch-vslam-node",children:"Launch VSLAM Node"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:"# isaac_ros_vslam.launch.py\n\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    vslam_node = Node(\n        package='isaac_ros_visual_slam',\n        executable='isaac_ros_visual_slam_node',\n        parameters=[\n            {'enable_imu_fusion': True},\n            {'enable_loop_closure': True},\n            {'num_keyframes': 10},\n            {'depth_image_topic': '/camera/depth'},\n            {'rgb_image_topic': '/camera/rgb'},\n            {'imu_topic': '/imu/data'},\n        ]\n    )\n\n    return LaunchDescription([vslam_node])\n"})}),"\n",(0,o.jsx)(n.h3,{id:"subscribe-to-odometry",children:"Subscribe to Odometry"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"from nav_msgs.msg import Odometry\nimport numpy as np\n\nclass LocalizationNode(Node):\n    def __init__(self):\n        super().__init__('localization_node')\n        self.odom_sub = self.create_subscription(\n            Odometry,\n            '/visual_slam/odometry',\n            self.odometry_callback,\n            10\n        )\n        self.robot_pose = None\n\n    def odometry_callback(self, msg):\n        # Extract pose\n        pos = msg.pose.pose.position\n        orient = msg.pose.pose.orientation\n\n        self.robot_pose = {\n            'x': pos.x, 'y': pos.y, 'z': pos.z,\n            'qx': orient.x, 'qy': orient.y, 'qz': orient.z, 'qw': orient.w\n        }\n\n        self.get_logger().info(f\"Robot at: ({pos.x:.2f}, {pos.y:.2f}, {pos.z:.2f})\")\n"})}),"\n",(0,o.jsx)(n.h2,{id:"real-time-object-detection",children:"Real-Time Object Detection"}),"\n",(0,o.jsx)(n.p,{children:"Use YOLOv8 for object detection:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"from isaac_ros_object_detection import DetectionNode\nfrom vision_msgs.msg import Detection2DArray\n\nclass ObjectDetector(Node):\n    def __init__(self):\n        super().__init__('object_detector')\n        self.det_sub = self.create_subscription(\n            Detection2DArray,\n            '/detections',\n            self.detection_callback,\n            10\n        )\n\n    def detection_callback(self, msg):\n        for detection in msg.detections:\n            # Extract bounding box and class\n            bbox = detection.bbox\n            label = detection.results[0].hypothesis.class_name\n            confidence = detection.results[0].hypothesis.score\n\n            if confidence > 0.7:  # High confidence\n                self.get_logger().info(\n                    f\"Detected {label} at ({bbox.center.x:.0f}, {bbox.center.y:.0f}) \"\n                    f\"(confidence: {confidence:.2f})\"\n                )\n"})}),"\n",(0,o.jsx)(n.h2,{id:"stereo-depth-estimation",children:"Stereo Depth Estimation"}),"\n",(0,o.jsx)(n.p,{children:"Estimate 3D structure from two cameras:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:"# stereo_depth.launch.py\n\ndef generate_launch_description():\n    stereo_depth_node = Node(\n        package='isaac_ros_dnn_stereo_depth',\n        executable='isaac_ros_dnn_stereo_depth_node',\n        parameters=[\n            {'stereo_baseline': 0.12},  # 120mm baseline\n            {'model': 'raft-stereo'},\n            {'image_width': 1280},\n            {'image_height': 720},\n        ]\n    )\n\n    return LaunchDescription([stereo_depth_node])\n"})}),"\n",(0,o.jsx)(n.h2,{id:"multi-sensor-fusion",children:"Multi-Sensor Fusion"}),"\n",(0,o.jsx)(n.p,{children:"Combine VSLAM, object detection, and depth for robust perception:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import tf_transformations\nfrom geometry_msgs.msg import TransformStamped\n\nclass PerceptionFusion(Node):\n    def __init__(self):\n        super().__init__(\'perception_fusion\')\n\n        # Subscribers\n        self.vslam_sub = self.create_subscription(Odometry, \'/visual_slam/odometry\', self.vslam_cb, 10)\n        self.det_sub = self.create_subscription(Detection2DArray, \'/detections\', self.det_cb, 10)\n        self.depth_sub = self.create_subscription(Image, \'/depth\', self.depth_cb, 10)\n\n        # Publisher\n        self.world_pub = self.create_publisher(Marker, \'/world_objects\', 10)\n\n    def vslam_cb(self, msg):\n        """Update robot localization."""\n        self.robot_frame = msg.pose.pose\n\n    def det_cb(self, msg):\n        """Detect objects in camera frame."""\n        for detection in msg.detections:\n            # Project 2D detection to 3D using depth map\n            x_pixel = detection.bbox.center.x\n            y_pixel = detection.bbox.center.y\n            depth = self.get_depth_at(x_pixel, y_pixel)\n\n            # Transform from camera to world frame\n            world_point = self.camera_to_world(depth, x_pixel, y_pixel)\n\n            # Publish marker\n            self.publish_marker(world_point, detection.results[0].hypothesis.class_name)\n\n    def get_depth_at(self, x, y):\n        """Get depth at pixel location."""\n        # Linear interpolation from depth map\n        # ...\n        pass\n\n    def camera_to_world(self, depth, x, y):\n        """Transform camera coordinates to world coordinates."""\n        # Use robot pose and camera intrinsics\n        # ...\n        pass\n'})}),"\n",(0,o.jsx)(n.h2,{id:"performance-metrics",children:"Performance Metrics"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import time\n\nclass PerformanceMonitor(Node):\n    def __init__(self):\n        super().__init__('perf_monitor')\n        self.timer = self.create_timer(1.0, self.monitor_callback)\n        self.frame_times = []\n\n    def monitor_callback(self):\n        avg_latency = np.mean(self.frame_times[-30:]) * 1000  # ms\n\n        self.get_logger().info(\n            f\"VSLAM latency: {avg_latency:.1f} ms ({1000/avg_latency:.1f} FPS)\"\n        )\n"})}),"\n",(0,o.jsx)(n.p,{children:"Expected performance on RTX 4090:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"VSLAM"}),": 4-5 ms per frame (200 FPS)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"YOLOv8"}),": 10-15 ms per frame (70 FPS)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Stereo depth"}),": 20-30 ms per frame (33 FPS)"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"next-lesson",children:"Next Lesson"}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.a,{href:"/docs/modules/m3-isaac/m3-nav2-planning",children:"Lesson 3: Nav2 Path Planning"})})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>i,x:()=>r});var t=s(6540);const o={},a=t.createContext(o);function i(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);