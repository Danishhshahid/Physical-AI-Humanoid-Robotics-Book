"use strict";(self.webpackChunkphysical_ai_humanoid_robotics_textbook=self.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[557],{3406:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>m,frontMatter:()=>s,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"modules/m4-vla/m4-voice-to-action","title":"Lesson 1: Voice-to-Action with Whisper","description":"Build real-time speech recognition for robots using OpenAI Whisper. Convert voice commands into actionable ROS 2 messages.","source":"@site/docs/modules/m4-vla/m4-voice-to-action.mdx","sourceDirName":"modules/m4-vla","slug":"/modules/m4-vla/m4-voice-to-action","permalink":"/docs/modules/m4-vla/m4-voice-to-action","draft":false,"unlisted":false,"editUrl":"https://github.com/Danishhshahid/Physical-AI-Humanoid-Robotics-Book/tree/main/website/docs/modules/m4-vla/m4-voice-to-action.mdx","tags":[],"version":"current","frontMatter":{"id":"m4-voice-to-action","title":"Lesson 1: Voice-to-Action with Whisper","sidebar_label":"L1: Voice-to-Action","description":"Build real-time speech recognition for robots using OpenAI Whisper. Convert voice commands into actionable ROS 2 messages."},"sidebar":"tutorialSidebar","previous":{"title":"M4: Overview","permalink":"/docs/modules/m4-vla/m4-overview"},"next":{"title":"L2: Cognitive Planning","permalink":"/docs/modules/m4-vla/m4-cognitive-planning"}}');var o=i(4848),r=i(8453);const s={id:"m4-voice-to-action",title:"Lesson 1: Voice-to-Action with Whisper",sidebar_label:"L1: Voice-to-Action",description:"Build real-time speech recognition for robots using OpenAI Whisper. Convert voice commands into actionable ROS 2 messages."},a=void 0,c={},d=[{value:"Speech Recognition Pipeline",id:"speech-recognition-pipeline",level:2},{value:"Installing Whisper",id:"installing-whisper",level:2},{value:"Local Whisper Node (ROS 2)",id:"local-whisper-node-ros-2",level:2},{value:"Intent Recognition",id:"intent-recognition",level:2},{value:"Extract Parameters from Speech",id:"extract-parameters-from-speech",level:2},{value:"Real-Time Performance",id:"real-time-performance",level:2},{value:"Next Lesson",id:"next-lesson",level:2}];function l(e){const n={a:"a",code:"code",h2:"h2",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h2,{id:"speech-recognition-pipeline",children:"Speech Recognition Pipeline"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'\ud83c\udfa4 Microphone Input (16 kHz, mono)\n    \u2193\n[Whisper: Audio \u2192 Text]\n    \u2193\n\ud83d\udcdd Text Command: "Move the arm left"\n    \u2193\n[Intent Classification]\n    \u2193\n\ud83c\udfaf ROS Action / Service Call\n'})}),"\n",(0,o.jsx)(n.h2,{id:"installing-whisper",children:"Installing Whisper"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Install with pip\npip install openai-whisper\n\n# Or from source\ngit clone https://github.com/openai/whisper.git\ncd whisper && pip install -e .\n\n# Verify\nwhisper --version\n"})}),"\n",(0,o.jsx)(n.h2,{id:"local-whisper-node-ros-2",children:"Local Whisper Node (ROS 2)"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport whisper\nimport numpy as np\nimport pyaudio\nimport threading\n\nclass WhisperNode(Node):\n    def __init__(self):\n        super().__init__(\'whisper_node\')\n\n        # Load Whisper model (base, small, medium, large)\n        # Larger = more accurate but slower\n        self.model = whisper.load_model("base")\n\n        # Publisher for recognized commands\n        self.command_pub = self.create_publisher(String, \'/voice/command\', 10)\n\n        # Audio recording parameters\n        self.CHUNK = 1024\n        self.FORMAT = pyaudio.paFloat32\n        self.CHANNELS = 1\n        self.RATE = 16000\n\n        self.audio_data = []\n        self.recording = False\n\n        # Start microphone thread\n        self.record_thread = threading.Thread(target=self.record_audio)\n        self.record_thread.daemon = True\n        self.record_thread.start()\n\n        self.get_logger().info("Whisper node started, listening for voice commands...")\n\n    def record_audio(self):\n        """Continuously record audio in background."""\n        p = pyaudio.PyAudio()\n\n        stream = p.open(\n            format=self.FORMAT,\n            channels=self.CHANNELS,\n            rate=self.RATE,\n            input=True,\n            frames_per_buffer=self.CHUNK\n        )\n\n        silence_duration = 0\n        SILENCE_THRESHOLD = 0.02\n        SILENCE_FRAMES = 30  # ~1.7 seconds of silence to end\n\n        while True:\n            data = stream.read(self.CHUNK)\n            audio_chunk = np.frombuffer(data, dtype=np.float32)\n\n            # Detect silence\n            rms = np.sqrt(np.mean(audio_chunk ** 2))\n\n            if rms < SILENCE_THRESHOLD:\n                silence_duration += 1\n            else:\n                silence_duration = 0\n                self.audio_data.extend(audio_chunk)\n\n            # If enough silence, process the utterance\n            if silence_duration > SILENCE_FRAMES and len(self.audio_data) > 0:\n                self.get_logger().info("End of speech detected, transcribing...")\n                self.transcribe_and_publish()\n                self.audio_data = []\n                silence_duration = 0\n\n    def transcribe_and_publish(self):\n        """Transcribe audio buffer and publish command."""\n        # Convert audio list to numpy array\n        audio_array = np.array(self.audio_data, dtype=np.float32)\n\n        # Normalize\n        if np.max(np.abs(audio_array)) > 0:\n            audio_array = audio_array / np.max(np.abs(audio_array))\n\n        # Transcribe with Whisper\n        try:\n            result = self.model.transcribe(\n                audio=audio_array,\n                language="en",\n                fp16=False  # Use float32\n            )\n\n            text = result["text"].strip()\n\n            if text:\n                self.get_logger().info(f"Transcribed: {text}")\n\n                # Publish command\n                msg = String()\n                msg.data = text\n                self.command_pub.publish(msg)\n            else:\n                self.get_logger().warn("No speech detected")\n\n        except Exception as e:\n            self.get_logger().error(f"Transcription error: {e}")\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = WhisperNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"intent-recognition",children:"Intent Recognition"}),"\n",(0,o.jsx)(n.p,{children:"Classify voice commands into actions:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"from enum import Enum\n\nclass RobotAction(Enum):\n    MOVE_ARM = \"move_arm\"\n    GRASP = \"grasp\"\n    RELEASE = \"release\"\n    WALK = \"walk\"\n    STOP = \"stop\"\n\nclass IntentRecognizer(Node):\n    def __init__(self):\n        super().__init__('intent_recognizer')\n\n        self.voice_sub = self.create_subscription(\n            String,\n            '/voice/command',\n            self.command_callback,\n            10\n        )\n\n        self.action_pub = self.create_publisher(String, '/robot/action', 10)\n\n    def command_callback(self, msg):\n        \"\"\"Parse voice command and extract intent.\"\"\"\n        text = msg.data.lower()\n\n        # Simple keyword-based classification\n        action = self.classify_command(text)\n\n        if action:\n            self.get_logger().info(f\"Recognized action: {action.value}\")\n            action_msg = String()\n            action_msg.data = action.value\n            self.action_pub.publish(action_msg)\n        else:\n            self.get_logger().warn(f\"Unknown command: {text}\")\n\n    def classify_command(self, text: str):\n        \"\"\"Classify command text to robot action.\"\"\"\n\n        move_keywords = ['move', 'go', 'left', 'right', 'forward', 'backward']\n        if any(kw in text for kw in move_keywords):\n            return RobotAction.MOVE_ARM\n\n        grasp_keywords = ['pick', 'grab', 'grasp', 'hold']\n        if any(kw in text for kw in grasp_keywords):\n            return RobotAction.GRASP\n\n        release_keywords = ['drop', 'release', 'let go', 'place']\n        if any(kw in text for kw in release_keywords):\n            return RobotAction.RELEASE\n\n        walk_keywords = ['walk', 'move', 'navigate', 'go to']\n        if any(kw in text for kw in walk_keywords):\n            return RobotAction.WALK\n\n        stop_keywords = ['stop', 'halt', 'freeze']\n        if any(kw in text for kw in stop_keywords):\n            return RobotAction.STOP\n\n        return None\n"})}),"\n",(0,o.jsx)(n.h2,{id:"extract-parameters-from-speech",children:"Extract Parameters from Speech"}),"\n",(0,o.jsx)(n.p,{children:"Parse parameters from voice commands:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import re\n\nclass CommandParser(Node):\n    def parse_movement_command(self, text: str) -> dict:\n        \"\"\"Extract movement parameters from command.\"\"\"\n\n        # \"Move arm 10 cm left\"\n        command = {'action': 'move_arm', 'direction': None, 'distance': None}\n\n        # Extract direction\n        directions = {'left': -1, 'right': 1, 'up': 1, 'down': -1, 'forward': 1, 'backward': -1}\n        for direction, sign in directions.items():\n            if direction in text:\n                command['direction'] = direction\n                break\n\n        # Extract distance (if mentioned)\n        distance_match = re.search(r'(\\d+)\\s*(cm|centimeter|meter|m)', text)\n        if distance_match:\n            distance = int(distance_match.group(1))\n            unit = distance_match.group(2)\n            if 'cm' in unit:\n                command['distance'] = distance / 100  # Convert to meters\n            else:\n                command['distance'] = distance\n\n        return command\n"})}),"\n",(0,o.jsx)(n.h2,{id:"real-time-performance",children:"Real-Time Performance"}),"\n",(0,o.jsx)(n.p,{children:"Whisper models and latency:"}),"\n",(0,o.jsxs)(n.table,{children:[(0,o.jsx)(n.thead,{children:(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.th,{children:"Model"}),(0,o.jsx)(n.th,{children:"Size"}),(0,o.jsx)(n.th,{children:"Latency"}),(0,o.jsx)(n.th,{children:"Accuracy"})]})}),(0,o.jsxs)(n.tbody,{children:[(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"tiny"})}),(0,o.jsx)(n.td,{children:"39 MB"}),(0,o.jsx)(n.td,{children:"100 ms"}),(0,o.jsx)(n.td,{children:"87%"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"base"})}),(0,o.jsx)(n.td,{children:"140 MB"}),(0,o.jsx)(n.td,{children:"200 ms"}),(0,o.jsx)(n.td,{children:"92%"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"small"})}),(0,o.jsx)(n.td,{children:"244 MB"}),(0,o.jsx)(n.td,{children:"400 ms"}),(0,o.jsx)(n.td,{children:"94%"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"medium"})}),(0,o.jsx)(n.td,{children:"769 MB"}),(0,o.jsx)(n.td,{children:"800 ms"}),(0,o.jsx)(n.td,{children:"96%"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"large"})}),(0,o.jsx)(n.td,{children:"2.9 GB"}),(0,o.jsx)(n.td,{children:"1500 ms"}),(0,o.jsx)(n.td,{children:"99%"})]})]})]}),"\n",(0,o.jsxs)(n.p,{children:["For real-time humanoid control, use ",(0,o.jsx)(n.strong,{children:"base"})," or ",(0,o.jsx)(n.strong,{children:"small"})," on edge devices."]}),"\n",(0,o.jsx)(n.h2,{id:"next-lesson",children:"Next Lesson"}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.a,{href:"/docs/modules/m4-vla/m4-cognitive-planning",children:"Lesson 2: Cognitive Planning with LLMs"})})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(l,{...e})}):l(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>a});var t=i(6540);const o={},r=t.createContext(o);function s(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);